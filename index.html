<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>OpenAI Projects</title>
    <link rel="stylesheet" type="text/css" href="main.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
</head>
<body>
<div id="header"></div>
<div id="body" class="container-fluid d-flex" style="padding-top: 10px; padding-bottom: 10px;">
    <div id="QLearning" class="row" style="margin: 0;">
        <div class="col-md-6 description">
            <h1>Q-learning</h1>
            <h3>Theory</h3>
            <p> One of the most basic forms of reinforcement learning we can implement is Q-learning. In an environment where there is a
                limited amount of possible states and actions, we assign a q-value to each possible state-action combination and store
                these values in a q-table. The q-values express how 'good' a certain action is in a certain state. The optimal action to
                perform in a state is then the one with the highest q-value. All a rational agent then has to do is to iteratively
                look up and perform the optimal action given his current state.</p>
            <p> But how do we find these q-values? Intuitively, we can reason that the utility of performing an action <i>a</i> in a state <i>s</i> is
                linked to the immediate reward of performing that action and the utility of the resulting state <i>s'</i>.
                It is also reasonable to assign less utility to the future rewards than to the immediate reward. To express this temporal
                value we multiply the future rewards with a discount factor <i>γ</i> valued between 0 and 1.
                Finally, if we assume our agent always selects the best action in any given state, we get the resulting Bellman equation:</p>
            <img class="equation" src="resources/images/BellmanEquation.png">
            <p> We can use the q-values stored in our table to perform this equation. However, we don't want to drastically change our q-values
                every time we visit an action-state pair. Instead, we want them to gradually evolve towards the optimal values. To do this we introduce another
                hyperparameter α, the learning rate. An update of a q-value for a state <i>s</i> and action <i>a</i> is then expressed as:</p>
            <img class="equation" src="resources/images/QUpdateEquation.png">
            <h3>Practice</h3>
            <h5>The challenge...</h5>
            <p> The environment I chose to implement Q-learning on is the <a href="https://gym.openai.com/envs/Taxi-v3/">Taxi-v3</a>
                (<a href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py">github</a>) environment from the
                <a href="http://gym.openai.com/">gym toolkit</a>. The state space consists of a 5x5 grid with 4 locations that can either be a passenger waiting
                location or a drop-off point. A passenger can either be on one of the 4 points, or inside the taxi. This brings us to a total of <i>500=5x5x4x(4+1)</i>
                possible states. The agent can choose between 6 actions: <i>pickup, dropoff, south, north, east, west</i>. At each timestep, the agent's score decreases
                by 1, forcing the agent to act promptly. A successful delivery of the passenger is reward with +20 points. An illegal attempt of the <i>pickup</i> or
                <i>dropoff</i> action is penalized with -10 points. The environment ends when the passenger is successfully dropped off.</p>
            <h5>The Solution... (<a href="https://github.com/YannouRavoet/gymprojects/blob/master/Taxi_v3.py">github</a>)</h5>
            <p> We first initialize a q-table of 6 rows by 500 columns that defines for each state how useful each action is. Since we have no clue how useful each
                action is, we just initialize all values to 1. This means, we will first need to explore our environment a bit, before our agent can create the optimal
                policy. To do this, we define an <i>exploration rate</i> that is initially set to 1 and steadily decreases to a small value such as 0.1. This value
                defines the percentage of time the agent will take a random value, versus the percentage of time it will take the optimal value (calculated based on the
                current states of its q-table). Finally, a learning rate of 0.1 and a discount factor of 0.6 are chosen and we simply let the agent train during a large number
                of iterations (for example 100k iterations). This way we ensure the agent will pass every state at least a couple of times and update the q-values accordingly.</p>
        </div>
        <div class="col-md-6 support">
        </div>
    </div>
    <div id="DeepQ-Network" class="row" style="margin: 0;">

        <div class="col-md-6 description">
            <h1>Deep Q-Network</h1>
            <h3>Theory</h3>
            <p>Q-learning is all good and well, but what if the state space is continuous. In that case we want a function that can generalize over unseen values. This function
               can be modelled as a deep neural network, hence the name Deep Q-Network, DQN. To increase the training efficiency, we train the network in batches, doing all calculations
               of q-values on a set of examples, and then using those examples and there corresponding q-values to update the weights of the network. In order to reduce the correlation
               between the examples in each batch we implement an experience memory buffer that stores all the agent's experiences. Every time we want to train a batch, we randomly select
               a set of experiences from this memory.</p>
            <h3>Practice</h3>
            <h5>The challenge...</h5>
            <p> The <a href="https://gym.openai.com/envs/CartPole-v0/">CartPole-v0</a> (<a href="https://github.com/openai/gym/wiki/CartPole-v0">github</a>) environment describes its state
                space with four continuous values: <i>cart position, cart velocity, pole angle</i> and <i>pole velocity at tip</i>. The CartPole environment has an extremely complex set of
                possible actions: <i>right</i> and <i>left</i>. As such, we want to create a function that takes 4 continuous outputs and tells our agent whether to go right or left.</p>
            <h5>The solution... (<a href="https://github.com/YannouRavoet/gymprojects/blob/master/Cartpole_v0.py">github</a>)</h5>
            <p> The first thing to do, is to create a neural network structure. We know we have a 4 node input and we can create a 2 node input, where we simply select the node with the highest output.
                In between these we create two 24 node layers (with a ReLu activation). Since our output layers use a simple linear activation function, we use a mean squared error loss function.
                Next, we need to implement our memory buffer. To do this, we create a deque with a max length of say... 1 million examples. We choose a batch size of 32 and in the same manner as with the
                Taxi-v3 environment, we start with an exploration rate of 1 that decays over time to 0.001.</p>
        </div>
        <div class="col-md-6 support">
        </div>
    </div>
    <div id="DoubleDeepQ-Network" class="row" style="margin: 0;">
        <div class="col-md-6 description">
            <h1>Double DQN</h1>
            <p>Test this and that</p>
        </div>
        <div class="col-md-6 support">
        </div>
    </div>
</div>
<div id="footer"> </div>

</body>
</html>